{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "from gensim import utils\n",
    "import torch\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredient types</th>\n",
       "      <th>diets</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>vegan</th>\n",
       "      <th>glutenFree</th>\n",
       "      <th>dairyFree</th>\n",
       "      <th>veryHealthy</th>\n",
       "      <th>cheap</th>\n",
       "      <th>veryPopular</th>\n",
       "      <th>sustainable</th>\n",
       "      <th>healthScore</th>\n",
       "      <th>pricePerServing</th>\n",
       "      <th>readyInMinutes</th>\n",
       "      <th>servings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>orange fig teacake with caramel glaze</td>\n",
       "      <td>orange fig teacake with caramel glaze is a veg...</td>\n",
       "      <td>you will need a 9  springform pan  or a cake ...</td>\n",
       "      <td>ap flour; baking powder; cardamom; eggs; fresh...</td>\n",
       "      <td>Beverages Milk Eggs Other Dairy Spices and Sea...</td>\n",
       "      <td>lacto ovo vegetarian</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.55</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>poached eggs on a bed of fried mushrooms and c...</td>\n",
       "      <td>poached eggs on a bed of fried mushrooms and c...</td>\n",
       "      <td>in a frying pan heat up oil then add mushroom...</td>\n",
       "      <td>bread; butter; eggs; eggs; mushrooms; oil; sal...</td>\n",
       "      <td>Beverages Milk Eggs Other Dairy Spices and Sea...</td>\n",
       "      <td>lacto ovo vegetarian</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>147.70</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pandan chiffon cake</td>\n",
       "      <td>for 26 cents per serving   this recipe covers ...</td>\n",
       "      <td>preheat the oven to 170c  blend the pandan le...</td>\n",
       "      <td>all purpose flour; bay leaves; coconut milk; c...</td>\n",
       "      <td>Ethnic Foods Produce Spices and Seasonings Bev...</td>\n",
       "      <td>dairy free; lacto ovo vegetarian</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.06</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>pork chop with honey  mustard and apples</td>\n",
       "      <td>pork chop with honey  mustard and apples might...</td>\n",
       "      <td>pre heat your oven to 200c   400f  line a roa...</td>\n",
       "      <td>apples; dijon mustard; garlic cloves; honey; j...</td>\n",
       "      <td>Meat Spices and Seasonings Condiments Oil Vine...</td>\n",
       "      <td>gluten free; dairy free; paleolithic; primal</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17.0</td>\n",
       "      <td>242.23</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>beet gnocchi with steak and brown butter sauce</td>\n",
       "      <td>the recipe beet gnocchi with steak and brown b...</td>\n",
       "      <td>cooking beets heat oven to 400 degrees wash be...</td>\n",
       "      <td>gnocchi; beets; olive oil; s p; goat cheese; r...</td>\n",
       "      <td>Produce Spices and Seasonings Meat Spices and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>417.69</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0              orange fig teacake with caramel glaze   \n",
       "1           1  poached eggs on a bed of fried mushrooms and c...   \n",
       "2           2                                pandan chiffon cake   \n",
       "3           3           pork chop with honey  mustard and apples   \n",
       "4           4     beet gnocchi with steak and brown butter sauce   \n",
       "\n",
       "                                             summary  \\\n",
       "0  orange fig teacake with caramel glaze is a veg...   \n",
       "1  poached eggs on a bed of fried mushrooms and c...   \n",
       "2  for 26 cents per serving   this recipe covers ...   \n",
       "3  pork chop with honey  mustard and apples might...   \n",
       "4  the recipe beet gnocchi with steak and brown b...   \n",
       "\n",
       "                                        instructions  \\\n",
       "0   you will need a 9  springform pan  or a cake ...   \n",
       "1   in a frying pan heat up oil then add mushroom...   \n",
       "2   preheat the oven to 170c  blend the pandan le...   \n",
       "3   pre heat your oven to 200c   400f  line a roa...   \n",
       "4  cooking beets heat oven to 400 degrees wash be...   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  ap flour; baking powder; cardamom; eggs; fresh...   \n",
       "1  bread; butter; eggs; eggs; mushrooms; oil; sal...   \n",
       "2  all purpose flour; bay leaves; coconut milk; c...   \n",
       "3  apples; dijon mustard; garlic cloves; honey; j...   \n",
       "4  gnocchi; beets; olive oil; s p; goat cheese; r...   \n",
       "\n",
       "                                    ingredient types  \\\n",
       "0  Beverages Milk Eggs Other Dairy Spices and Sea...   \n",
       "1  Beverages Milk Eggs Other Dairy Spices and Sea...   \n",
       "2  Ethnic Foods Produce Spices and Seasonings Bev...   \n",
       "3  Meat Spices and Seasonings Condiments Oil Vine...   \n",
       "4  Produce Spices and Seasonings Meat Spices and ...   \n",
       "\n",
       "                                          diets  vegetarian  vegan  \\\n",
       "0                          lacto ovo vegetarian        True  False   \n",
       "1                          lacto ovo vegetarian        True  False   \n",
       "2              dairy free; lacto ovo vegetarian        True  False   \n",
       "3  gluten free; dairy free; paleolithic; primal       False  False   \n",
       "4                                           NaN       False  False   \n",
       "\n",
       "   glutenFree  dairyFree  veryHealthy  cheap  veryPopular  sustainable  \\\n",
       "0       False      False        False  False        False        False   \n",
       "1       False      False        False  False        False        False   \n",
       "2       False       True        False  False        False        False   \n",
       "3        True       True        False  False        False        False   \n",
       "4       False      False        False  False        False        False   \n",
       "\n",
       "   healthScore  pricePerServing  readyInMinutes  servings  \n",
       "0          3.0            75.55              45        10  \n",
       "1         15.0           147.70              45         2  \n",
       "2          1.0            26.06              45         9  \n",
       "3         17.0           242.23              45         4  \n",
       "4         12.0           417.69              45         4  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('cleaneddata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             0\n",
       "title                  0\n",
       "summary                5\n",
       "instructions         201\n",
       "ingredients            0\n",
       "ingredient types       0\n",
       "diets               1078\n",
       "vegetarian             0\n",
       "vegan                  0\n",
       "glutenFree             0\n",
       "dairyFree              0\n",
       "veryHealthy            0\n",
       "cheap                  0\n",
       "veryPopular            0\n",
       "sustainable            0\n",
       "healthScore            0\n",
       "pricePerServing        0\n",
       "readyInMinutes         0\n",
       "servings               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each column, check how many NAN values exist\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "# 1. For each input recipe, have a result vector of it's properties\n",
    "# 2. For example, [isVegetarian, isVegan, isGlutenFree, isDairyFree, isLowSugar, isVeryHealthy, isCheap, isVeryPopular, isSustainable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all \"possible\" ingredients\n",
    "ingredients = data[\"ingredients\"].to_list()\n",
    "\n",
    "all_ingredients = []\n",
    "for i in ingredients: \n",
    "    items = i.split(\"; \")\n",
    "    all_ingredients += items\n",
    "\n",
    "all_ingredients = list(set(all_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ap-flour', 'baking-powder', 'cardamom', 'eggs', 'fresh-figs', 'heavy-cream', 'navel-oranges', 'salt', 'sugar', 'unsalted-butter', 'vanilla-extract', 'water'], ['bread', 'butter', 'eggs', 'eggs', 'mushrooms', 'oil', 'salt', 'salt', 'vinegar', 'water'], ['all-purpose-flour', 'bay-leaves', 'coconut-milk', 'corn-oil', 'cream-of-tartar', 'curry-paste', 'egg-whites', 'egg-yolks', 'salt', 'sugar', 'water'], ['apples', 'dijon-mustard', 'garlic-cloves', 'honey', 'juice-of-lemon', 'olive-oil', 'pork-chops', 'salt-and-pepper', 'white-onion'], ['gnocchi', 'beets', 'olive-oil', 's-p', 'goat-cheese', 'ricotta', 'flour', 'steak', 'butter', 'shallot', 'butter', 'fresh-thyme', 'walnuts']]\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# GET X and Y\n",
    "\n",
    "# For each row in the datframe data\n",
    "propertyColumns = ['vegetarian', 'vegan', 'glutenFree', 'dairyFree', 'veryHealthy', 'cheap', 'veryPopular', 'sustainable']\n",
    "\n",
    "Y = torch.zeros(len(data), len(propertyColumns))\n",
    "X = []\n",
    "\n",
    "# For each recipe, get a list of ingredients (input) and a \n",
    "# binary vector of the properties (output)\n",
    "for i, row in data.iterrows():\n",
    "    # Get the ingredients for each recipe\n",
    "    ingredient = row['ingredients'].split(\"; \")\n",
    "    # For each ingredient, replace whitespaces with a hyphen\n",
    "    ingredient = [i.replace(\" \", \"-\") for i in ingredient]\n",
    "    X.append(ingredient)\n",
    "    \n",
    "    # Get a binary vector for each property to be predicted\n",
    "    for j in range(len(propertyColumns)):\n",
    "        columnName = propertyColumns[j]\n",
    "        if row[columnName] == True:\n",
    "            Y[i][j] = 1\n",
    "\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "assert len(X) == len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X to Word2Vec\n",
    "\n",
    "# Tokenize, lowercase, and remove punctuation, numbers, and alphanumeric characters\n",
    "w2vec = gs.models.Word2Vec(X, vector_size=100, workers=4)\n",
    "w2vec.save(\"w2vec_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 100)\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "word_vectors = w2vec.wv\n",
    "all_ingredients = word_vectors.key_to_index\n",
    "embeddings = word_vectors.vectors\n",
    "print(embeddings.shape)\n",
    "print(len(all_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4299,)\n",
      "(4299, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-139-fc00c79c82cd>:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(X_final)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filter X, Y to only keep those ingredients in X that have embeddings\n",
    "X_final = []\n",
    "Y_final = []\n",
    "for i in range(len(X)):\n",
    "    x = [a for a in X[i] if a in all_ingredients]\n",
    "    if x != []:\n",
    "        X_final.append(x)\n",
    "        Y_final.append(np.array(Y[i]))\n",
    "\n",
    "X = np.array(X_final)\n",
    "Y = np.array(Y_final)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3439, 100)\n",
      "(3439, 8)\n",
      "[-0.18059078  0.34228677 -0.30207455 -0.00533764 -0.0374865  -0.47131056\n",
      "  0.37753576  0.18378316 -0.79107     0.09286666 -0.20051491 -0.395642\n",
      "  0.16111849  0.382102   -0.08139008 -0.41169694 -0.09023887 -0.16439267\n",
      "  0.11145539 -0.59541297  0.38597038  0.00894879  0.25511956 -0.28313038\n",
      "  0.17430067  0.06952149 -0.1828475  -0.30113685 -0.13362797 -0.22394526\n",
      "  0.45854896  0.11858353  0.22010475  0.11835691 -0.13748641  0.39917758\n",
      " -0.04176695  0.04680825  0.19782577 -0.85688454 -0.11450142 -0.12954219\n",
      "  0.30807334 -0.17557888  0.40472862  0.0344989  -0.29920813 -0.1677821\n",
      "  0.21044278 -0.17289107  0.3569338  -0.21477485  0.05858085  0.13258383\n",
      "  0.00506248 -0.0622467  -0.03926927  0.22899216 -0.72934157 -0.03464407\n",
      "  0.1706075   0.23083025  0.00419992 -0.1284201  -0.71663797  0.0905507\n",
      "  0.09400386  0.03550544 -0.4046216   0.66603    -0.4205427   0.08438339\n",
      "  0.31017599  0.07598463 -0.03003437  0.05058973 -0.09639854  0.22768298\n",
      " -0.1381442   0.39992276 -0.23375443 -0.19788864 -0.5942415   0.24577783\n",
      "  0.13908988 -0.09798965 -0.00721678  0.41239196  0.6243273   0.12060931\n",
      " -0.09974197  0.42488918  0.02343052 -0.02079851  0.7235001   0.33370972\n",
      " -0.10790424 -0.40341207  0.352658    0.07376835]\n",
      "[1. 1. 1. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Trained word2vec. \n",
    "# Now, train a neural network to predict properties\n",
    "model = MLPClassifier(hidden_layer_sizes=(50, 25, 10), solver= \"adam\", max_iter=1000, random_state=1)\n",
    "\n",
    "# Training the model\n",
    "# For each recipe, get the average embedding of its ingredient list\n",
    "X_embedding_train = []\n",
    "for recipe in X_train:\n",
    "    for ingredient in recipe:\n",
    "        e = []\n",
    "        try:\n",
    "            a = np.array(w2vec.wv[ingredient])\n",
    "            e.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    e = np.array(e)\n",
    "    avg_emb = np.average(e, axis=0)\n",
    "\n",
    "    X_embedding_train.append(avg_emb)\n",
    "    \n",
    "    \n",
    "X_embedding_train = np.array(X_embedding_train)\n",
    "y_train = np.array(y_train)\n",
    "print(X_embedding_train.shape)\n",
    "print(y_train.shape)\n",
    "# model.fit(X_embedding_train, y_train)\n",
    "print(X_embedding_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "# prediction = model.predict(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, I have an input matrix and a label vector. Why am I not able to train the model?\n",
    "model.fit(X_embedding_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedding_test = []\n",
    "for recipe in X_test:\n",
    "    for ingredient in recipe:\n",
    "        e = []\n",
    "        try:\n",
    "            a = np.array(w2vec.wv[ingredient])\n",
    "            e.append(a)\n",
    "        except:\n",
    "            pass\n",
    "    e = np.array(e)\n",
    "    avg_emb = np.average(e, axis=0)\n",
    "\n",
    "    X_embedding_test.append(avg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_embedding_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1313953488372093\n",
      "0.19912790697674418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "\n",
    "score = accuracy_score(y_test, prediction)\n",
    "test_loss = hamming_loss(y_test, prediction)\n",
    "\n",
    "print(score)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19326112241930793\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-fa4e76855fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_1.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "training_pred = model.predict(X_embedding_train)\n",
    "\n",
    "training_loss = hamming_loss(y_train, training_pred)\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [list(['ground-flaxseed', 'water', 'wheat-bran', 'bran', 'spelt', 'spelt-flour', 'psyllium-husks', 'stevia-extract', 'flaxseeds', 'chia-seeds', 'baking-soda', 'sea-salt', 'nutmeg', 'cinnamon', 'demerara-sugar', 'agave-nectar', 'applesauce', 'ener-g-egg-replacer', 'milk', 'canola-oil', 'raisins', 'prunes'])\n",
      " list(['peanut-butter', 'bananas', 'cocoa-powder', 'agave-nectar', 'vanilla-extract'])\n",
      " list(['coconut-flour', 'baking-powder', 'baking-soda', 'dark-brown-sugar', 'kosher-salt', 'unsweetened-shredded-coconut', 'eggs', 'coconut-oil', 'coconut-milk', 'vanilla-extract', 'blueberries'])\n",
      " list(['mayonnaise', 'capers', 'horseradish', 'dijon-mustard', 'shallot', 'parsley', 'cod-fillets', 'bay-leaves', 'milk', 'water', 'potatoes', 'salt', 'lemon-zest', 'fresh-parsley', 'chives', 'pepper', 'flour', 'egg', 'breadcrumbs', 'sunflower-oil', 'lemon-wedges'])\n",
      " list(['apricot', 'baby-spinach', 'button-mushrooms', 'chili-paste', 'dried-apricots', 'fettuccini', 'garlic', 'olive-oil', 'romano-cheese', 'walnuts', 'water'])]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "\n",
      "\n",
      "Actual:  ['vegetarian']\n",
      "Actual:  ['glutenFree', 'dairyFree']\n",
      "Actual:  []\n",
      "Actual:  []\n",
      "Actual:  ['vegetarian', 'vegan', 'glutenFree', 'dairyFree']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Examples of Predictions\n",
    "\n",
    "random_index = random.sample(range(0, len(X_test)), 5)\n",
    "# In training dataset\n",
    "print(\"Input: \", X_train[random_index])\n",
    "\n",
    "# Get predicted properties\n",
    "# For each recipe\n",
    "y_pred = model.predict(X_embedding_train[random_index])\n",
    "for j in range(len(y_pred)):\n",
    "    # For each property\n",
    "    p = []\n",
    "    for k in range(len(y_pred[j])):\n",
    "        print(y_pred)\n",
    "        if y_pred[j][k] == 1:\n",
    "            p.append(propertyColumns[k])\n",
    "    print(\"Predicted: \", p)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get actual properties\n",
    "y_true = y_train[random_index]\n",
    "for j in range(len(y_true)):\n",
    "    p = []\n",
    "    for k in range(len(y_true[j])):\n",
    "        if y_true[j][k] == 1:\n",
    "            p.append(propertyColumns[k])\n",
    "    print(\"Actual: \", p)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# In test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [list(['bay-leaves', 'canned-beans', 'canned-tomato-sauce', 'canned-tomatoes', 'carrots', 'cayenne-pepper', 'celery', 'chili-powder', 'corn', 'cumin', 'ears-corn', 'garlic', 'green-bell-pepper', 'onion', 'oregano', 'salt', 'tomatoes', 'vegetable-stock'])\n",
      " list(['baking-soda', 'baking-powder', 'salt', 'cinnamon', 'ground-nutmeg', 'applesauce', 'vanilla', 'egg-whites', 'coconut-oil', 'cranberries', 'pecans'])\n",
      " list(['bananas', 'juice-of-orange', 'crackers', 'semi-sweet-chocolate-baking-chips', 'margarine', 'walnuts', 'no-calorie-sweetener', 'pb-cups'])\n",
      " list(['all-purpose-flour', 'baking-powder', 'cherries', 'eggs', 'granulated-sugar', 'milk', 'unsalted-butter', 'vanilla-sugar'])\n",
      " list(['peanut-oil', 'cremini-mushrooms', 'ground-turkey', 'black-pepper', 'ground-cinnamon', 'allspice', 'ground-ginger', 'ground-coriander', 'garlic', 'soy-sauce', 'rice-wine', 'oyster-sauce', 'granny-smith-apple', 'scallions', 'hoisin-sauce'])]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Predicted:  ['glutenFree']\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Predicted:  []\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Predicted:  []\n",
      "\n",
      "\n",
      "Actual:  ['glutenFree', 'veryHealthy']\n",
      "Actual:  ['vegetarian', 'glutenFree']\n",
      "Actual:  ['vegetarian']\n",
      "Actual:  []\n",
      "Actual:  ['vegetarian', 'vegan', 'glutenFree', 'dairyFree', 'veryHealthy']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \", X_test[random_index])\n",
    "random_index = random.sample(range(0, len(X_test)), 5)\n",
    "\n",
    "X_embedding_test = np.array(X_embedding_test)\n",
    "# Get predicted properties\n",
    "# For each recipe\n",
    "y_pred = model.predict(X_embedding_test[random_index])\n",
    "for j in range(len(y_pred)):\n",
    "    # For each property\n",
    "    p = []\n",
    "    for k in range(len(y_pred[j])):\n",
    "        print(y_pred)\n",
    "        if y_pred[j][k] == 1:\n",
    "            p.append(propertyColumns[k])\n",
    "    print(\"Predicted: \", p)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get actual properties\n",
    "y_true = y_test[random_index]\n",
    "for j in range(len(y_true)):\n",
    "    p = []\n",
    "    for k in range(len(y_true[j])):\n",
    "        if y_true[j][k] == 1:\n",
    "            p.append(propertyColumns[k])\n",
    "    print(\"Actual: \", p)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
